# -*- coding: utf-8 -*-
"""Dicoding_ML Terapan_Submission Pertama_Mobile Price Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AmJ2Dc73QBf2bqU_c_pmZEGjKreTLf3I

### Initial Setup
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Models
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

train_dataset = pd.read_csv('/content/drive/MyDrive/Dataset/Regression and Prediction/Mobile Price/train.csv')

"""### Exploratory Data Analysis

#### Whole Dataset
"""

train_dataset.tail()

"""The train dataset has 2000 rows with 21 columns (20 independent and 1 dependent variable)"""

train_dataset.info()

"""There are no missing value from each columns"""

train_dataset.describe()

print(train_dataset.shape)

train_dataset.iloc[0, :]

# Separate continuous and binary data

bin_features = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']
int_features = [x for x in train_dataset.columns if x not in bin_features]

"""#### Univariate Analysis"""

train_dataset['price_range'].value_counts().plot(kind='bar')

"""Fortunately, the dataset has a balanced labels

#### binary features

To understand the correlation between categorical features and the price range
"""

bin_features

"""> **blue**"""

sns.histplot(data=train_dataset, x=bin_features[0], hue=train_dataset['price_range'])

train_dataset[['blue', 'price_range']].value_counts().plot(kind='bar')

"""> **dual_sim**"""

sns.histplot(data=train_dataset, x=bin_features[1], hue=train_dataset['price_range'])

train_dataset[['dual_sim', 'price_range']].value_counts().plot(kind='bar')

"""> **four_g**"""

sns.histplot(data=train_dataset, x=bin_features[2], hue=train_dataset['price_range'])

train_dataset[['four_g', 'price_range']].value_counts().plot(kind='bar')

"""> **three_g**"""

sns.histplot(data=train_dataset, x=bin_features[3], hue=train_dataset['price_range'])

train_dataset[['three_g', 'price_range']].value_counts().plot(kind='bar')

"""> **touch_screen**"""

sns.histplot(data=train_dataset, x=bin_features[4], hue=train_dataset['price_range'])

train_dataset[['touch_screen', 'price_range']].value_counts().plot(kind='bar')

"""> **wifi**"""

sns.histplot(data=train_dataset, x=bin_features[5], hue=train_dataset['price_range'])

train_dataset[['wifi', 'price_range']].value_counts().plot(kind='bar')

"""**Conclusion**: there are no correlation between binary variables and the label. Therefore, we can drop the binary variables from the dataset.

#### Integer Variables
"""

sns.pairplot(train_dataset[int_features])

sns.heatmap(train_dataset[int_features].corr().round(1), annot=True)

"""**Conclusion**: From the plots (pairplot and heatmap), the only continuous features that have correlation with the label are **battery_power** (weak, 0.2), **px_height** (weak, 0.1), **px_width** (weak, 0.2), and **ram** (strong, 0.9). Therefore, we will try to use these features.

### Preprocessing
"""

# Take usable features

usable_variables = ['battery_power', 'px_height', 'px_width', 'ram', 'price_range']
df = train_dataset[usable_variables].copy()
df.tail()

# Train Test Split

X = df.drop(['price_range'], axis=1)
y = df['price_range']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

X_train

# Standardization

scaler = StandardScaler()
scaler.fit(X_train[:])
X_train[:] = scaler.transform(X_train.loc[:])
X_train[:].tail()

"""### Modelling

> **Test Data Preparation**
"""

# Scaling test set

X_test.loc[:] = scaler.transform(X_test[:])
X_test.tail()

"""> **Model Preparation**"""

# Initialize models

logreg_model = LogisticRegression()
svm_model = svm.SVC(kernel='linear')
rf_model = RandomForestClassifier()

# Fit the models with train data

logreg_model.fit(X_train, y_train)
svm_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)

# Logistic Regression

y_pred = logreg_model.predict(X_test)
print(classification_report(y_test, y_pred))

# SVM

y_pred = svm_model.predict(X_test)
print(classification_report(y_test, y_pred))

# Random Forest

y_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_pred))

"""**Conclusion**: Overall, **Logistic Regression** model reached the highest performance with 95% accuracy compared to other models (**SVM** (94%) and **Random Forest** (91%)). Therefore, **Logistic Regression** will be used as the best baseline model."""